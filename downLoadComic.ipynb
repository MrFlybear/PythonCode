{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬虫有学问，骚年还得多看书!!!!!!!!!!!!!!!!\n",
    "from selenium import webdriver\n",
    "import time,pprint,bs4,os\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import requests,threading\n",
    "import json,sys\n",
    "\n",
    "def getLatesturl():\n",
    "    os.chdir(r'C:\\Users\\xxcmt\\Desktop\\文档\\automate_online-materials')\n",
    "    res=requests.get('https://ac.qq.com/Comic/comicInfo/id/525099')\n",
    "    soup=bs4.BeautifulSoup(res.text,'html')\n",
    "    #检查是否为最新html \n",
    "    capterElem=soup.select('#chapter > div.works-chapter-top.subscribe-wrap > ul > li:nth-child(2) > a')\n",
    "    latestCapterName=capterElem[0].get_text()\n",
    "    with open(r'.\\comicPictures\\log.json','r') as load_f:\n",
    "        latestCapter_dict=json.load(load_f)\n",
    "    print('打开日志检查是否为最新章')\n",
    "    latestCapterName='恶偶'+latestCapterName#完善日志的key\n",
    "    if latestCapterName not in latestCapter_dict.keys():\n",
    "        latestComicurl='https://ac.qq.com/'+capterElem[0].get('href')\n",
    "        latestCapter_dict[latestCapterName]=latestComicurl\n",
    "        #将最新章的日志写入\n",
    "        with open(r'.\\comicPictures\\log.json','w') as f:\n",
    "            json.dump(latestCapter_dict,f,ensure_ascii=False)\n",
    "        eouFloder=os.path.join(r'C:\\Users\\xxcmt\\Desktop\\文档\\automate_online-materials\\comicPictures','恶偶',latestCapterName)\n",
    "        os.makedirs(eouFloder,exist_ok=True)\n",
    "        print(\"检测到最新章为%s，开始下载...\"%latestCapterName)\n",
    "        return latestComicurl,eouFloder\n",
    "    else:\n",
    "        print('已经为最新章节')\n",
    "        return -1,-1\n",
    "   \n",
    "lastUrl,picture_Floder=getLatesturl()\n",
    "if lastUrl!=-1:\n",
    "    browser = webdriver.Firefox()\n",
    "    browser.get(lastUrl)\n",
    "    time.sleep(2)\n",
    "    htmlElem = browser.find_element_by_tag_name('html')\n",
    "else:\n",
    "    os._exit(0)\n",
    "    \n",
    "#模拟pressDown的动作，并更新源码\n",
    "def pressPageDown():\n",
    "    htmlElem.send_keys(Keys.PAGE_DOWN)\n",
    "    htmlElem.send_keys(Keys.PAGE_DOWN)\n",
    "    htmlElem.send_keys(Keys.PAGE_DOWN)\n",
    "    htmlElem.send_keys(Keys.PAGE_DOWN) \n",
    "    time.sleep(2)\n",
    "    html_text=browser.page_source\n",
    "    os.chdir(r'C:\\Users\\xxcmt\\Desktop\\文档\\automate_online-materials')\n",
    "    with open('badidol.html','w',encoding='utf-8') as f:\n",
    "        f.write(html_text)\n",
    "            \n",
    "#读取源码，检查是否下载完\n",
    "increasePageNum={}\n",
    "def isCompleteDownLoad():\n",
    "    with open('badidol.html','r',encoding='utf-8') as f:\n",
    "        soup=bs4.BeautifulSoup(f,'html')\n",
    "    imgElems=soup.select('#comicContain  li  img')\n",
    "    for i in imgElems:\n",
    "        if i.get('src').startswith('https://manhua.qpic.cn/manhua_detail') is not True:\n",
    "            imgElems.remove(i)\n",
    "    print('已加载%s张图片'%len(imgElems))\n",
    "    #因为缓存的原因，所以容错机制连续4次press没有变化的情况，表示加载完毕\n",
    "    if len(imgElems) not in increasePageNum.keys():\n",
    "        increasePageNum[len(imgElems)]=0\n",
    "        return 0,imgElems\n",
    "    else:\n",
    "        increasePageNum[len(imgElems)]=increasePageNum[len(imgElems)]+1\n",
    "        if increasePageNum[len(imgElems)]==3:\n",
    "            return 1,imgElems\n",
    "        else:\n",
    "            return 0,imgElems\n",
    "           \n",
    "#先pressdown一次，再检查，如此往覆,返回完整的imgElems\n",
    "def downloadLazyComic():\n",
    "    pressPageDown()\n",
    "    isComplete,imgElems=isCompleteDownLoad()\n",
    "    while isComplete==0:\n",
    "        pressPageDown()\n",
    "        isComplete,imgElems=isCompleteDownLoad()\n",
    "    return imgElems\n",
    "\n",
    "#多线程下载\n",
    "imgElems=downloadLazyComic()\n",
    "browser.quit()\n",
    "\n",
    "os.chdir(picture_Floder)\n",
    "#分成5或者6个线程\n",
    "i=5\n",
    "while True:\n",
    "    step=len(imgElems)//i\n",
    "    if step==0:\n",
    "        i=i-1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "#下载从a到b的图像\n",
    "def downloadComic(a,b):\n",
    "    for i in range (a,b):\n",
    "        url=imgElems[i].get('src')\n",
    "        res=requests.get(url)\n",
    "        imageFile=open(str(i+1)+'.jpg','wb') \n",
    "        for chunk in res.iter_content(100000):\n",
    "            imageFile.write(chunk)\n",
    "        imageFile.close()\n",
    "        print('picture %s has downloaded'%(i+1))\n",
    "\n",
    "downloadThreads=[]\n",
    "for i in range(0,len(imgElems),step):\n",
    "    downloadThread=threading.Thread(target=downloadComic,args=(i,min(i+step,len(imgElems))))\n",
    "    downloadThreads.append(downloadThread)\n",
    "    downloadThread.start()\n",
    "for downloadThread in downloadThreads:\n",
    "    downloadThread.join()\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
